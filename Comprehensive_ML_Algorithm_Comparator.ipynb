{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification, make_regression, make_blobs\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    # Classification metrics\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, log_loss, confusion_matrix, classification_report,\n",
        "    balanced_accuracy_score, matthews_corrcoef, cohen_kappa_score,\n",
        "    # Regression metrics\n",
        "    mean_squared_error, mean_absolute_error, r2_score,\n",
        "    explained_variance_score, median_absolute_error, mean_absolute_percentage_error,\n",
        "    max_error, mean_poisson_deviance, mean_gamma_deviance,\n",
        "    # Clustering metrics\n",
        "    silhouette_score, calinski_harabasz_score, davies_bouldin_score,\n",
        "    adjusted_rand_score, normalized_mutual_info_score, homogeneity_score,\n",
        "    completeness_score, v_measure_score\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, RandomForestRegressor,\n",
        "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
        "    AdaBoostClassifier, AdaBoostRegressor, ExtraTreesClassifier, ExtraTreesRegressor\n",
        ")\n",
        "from sklearn.svm import SVC, SVR, LinearSVC, LinearSVR\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, Birch, SpectralClustering\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier, GaussianProcessRegressor\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import warnings\n",
        "import joblib\n",
        "from functools import lru_cache\n",
        "import concurrent.futures\n",
        "from scipy.stats import spearmanr\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to import XGBoost\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "    print(\"XGBoost not available. Install with: pip install xgboost\")\n",
        "\n",
        "# Set matplotlib to non-interactive backend for faster performance\n",
        "plt.switch_backend('Agg')\n",
        "\n",
        "class ComprehensiveMLComparator:\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.results_cache = {}\n",
        "        self._init_preconfigured_models()\n",
        "\n",
        "    def _init_preconfigured_models(self):\n",
        "        \"\"\"Pre-configure models for faster instantiation including XGBoost\"\"\"\n",
        "        self.preconfigured_models = {\n",
        "            'classification': {\n",
        "                'logistic_regression': LogisticRegression(C=1.0, solver='liblinear', max_iter=1000, n_jobs=-1),\n",
        "                'decision_tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "                'random_forest': RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1),\n",
        "                'extra_trees': ExtraTreesClassifier(n_estimators=50, random_state=42, n_jobs=-1),\n",
        "                'gradient_boosting': GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, random_state=42),\n",
        "                'ada_boost': AdaBoostClassifier(n_estimators=50, random_state=42),\n",
        "                'svm': SVC(C=1.0, kernel='rbf', random_state=42, probability=True),\n",
        "                'linear_svm': LinearSVC(C=1.0, random_state=42, max_iter=1000),\n",
        "                'knn': KNeighborsClassifier(n_neighbors=5, n_jobs=-1),\n",
        "                'naive_bayes': GaussianNB(),\n",
        "                'lda': LinearDiscriminantAnalysis(),\n",
        "                'qda': QuadraticDiscriminantAnalysis(),\n",
        "                'mlp': MLPClassifier(hidden_layer_sizes=(50,), max_iter=500, random_state=42),\n",
        "                'sgd': SGDClassifier(random_state=42, n_jobs=-1),\n",
        "                'gaussian_process': GaussianProcessClassifier(random_state=42) # Corrected this line\n",
        "            },\n",
        "            'regression': {\n",
        "                'linear_regression': LinearRegression(n_jobs=-1),\n",
        "                'ridge': Ridge(alpha=1.0, random_state=42),\n",
        "                'lasso': Lasso(alpha=1.0, random_state=42),\n",
        "                'elastic_net': ElasticNet(alpha=1.0, random_state=42),\n",
        "                'decision_tree': DecisionTreeRegressor(max_depth=5, random_state=42),\n",
        "                'random_forest': RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1),\n",
        "                'extra_trees': ExtraTreesRegressor(n_estimators=50, random_state=42, n_jobs=-1),\n",
        "                'gradient_boosting': GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, random_state=42),\n",
        "                'ada_boost': AdaBoostRegressor(n_estimators=50, random_state=42),\n",
        "                'svm': SVR(C=1.0, kernel='rbf'),\n",
        "                'linear_svr': LinearSVR(C=1.0, random_state=42, max_iter=1000),\n",
        "                'knn': KNeighborsRegressor(n_neighbors=5, n_jobs=-1),\n",
        "                'mlp': MLPRegressor(hidden_layer_sizes=(50,), max_iter=500, random_state=42),\n",
        "                'sgd': SGDRegressor(random_state=42),\n",
        "                'gaussian_process': GaussianProcessRegressor(random_state=42) # Corrected this line\n",
        "            },\n",
        "            'clustering': {\n",
        "                'kmeans': KMeans(n_clusters=3, random_state=42, n_init=10),\n",
        "                'dbscan': DBSCAN(eps=0.5, min_samples=5),\n",
        "                'agglomerative': AgglomerativeClustering(n_clusters=3),\n",
        "                'birch': Birch(n_clusters=3, threshold=0.5),\n",
        "                'spectral': SpectralClustering(n_clusters=3, random_state=42, n_jobs=-1)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Add XGBoost if available\n",
        "        if XGBOOST_AVAILABLE:\n",
        "            self.preconfigured_models['classification']['xgboost'] = xgb.XGBClassifier(\n",
        "                n_estimators=50, learning_rate=0.1, random_state=42, n_jobs=-1,\n",
        "                max_depth=3, subsample=0.8, colsample_bytree=0.8\n",
        "            )\n",
        "            self.preconfigured_models['regression']['xgboost'] = xgb.XGBRegressor(\n",
        "                n_estimators=50, learning_rate=0.1, random_state=42, n_jobs=-1,\n",
        "                max_depth=3, subsample=0.8, colsample_bytree=0.8\n",
        "            )\n",
        "\n",
        "    @lru_cache(maxsize=10)\n",
        "    def generate_sample_data(self, task_type, n_samples=1000, n_features=10, random_state=42):\n",
        "        \"\"\"Generate sample data for demonstration with caching\"\"\"\n",
        "        if task_type == \"classification\":\n",
        "            X, y = make_classification(\n",
        "                n_samples=n_samples, n_features=n_features,\n",
        "                n_informative=min(8, n_features), n_redundant=max(1, n_features//5),\n",
        "                n_classes=3, n_clusters_per_class=1, random_state=random_state\n",
        "            )\n",
        "            return X, y\n",
        "\n",
        "        elif task_type == \"regression\":\n",
        "            X, y = make_regression(\n",
        "                n_samples=n_samples, n_features=n_features,\n",
        "                noise=0.1, random_state=random_state\n",
        "            )\n",
        "            return X, y\n",
        "\n",
        "        elif task_type == \"clustering\":\n",
        "            X, y = make_blobs(\n",
        "                n_samples=n_samples, n_features=n_features,\n",
        "                centers=4, random_state=random_state, cluster_std=1.0\n",
        "            )\n",
        "            return X, y\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def preprocess_data(self, X, y=None, task_type=\"classification\"):\n",
        "        \"\"\"Preprocess the data efficiently\"\"\"\n",
        "        if task_type in [\"classification\", \"regression\"]:\n",
        "            # Use a smaller test size for faster evaluation\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=0.15, random_state=42, stratify=y if task_type == \"classification\" else None\n",
        "            )\n",
        "            X_train = self.scaler.fit_transform(X_train)\n",
        "            X_test = self.scaler.transform(X_test)\n",
        "            return X_train, X_test, y_train, y_test\n",
        "        else:\n",
        "            X_scaled = self.scaler.fit_transform(X)\n",
        "            return X_scaled, None, None, None\n",
        "\n",
        "    def calculate_classification_metrics(self, model, X_train, X_test, y_train, y_test):\n",
        "        \"\"\"Calculate comprehensive classification metrics\"\"\"\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(y_test, y_pred),\n",
        "            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
        "            'precision_macro': precision_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "            'precision_weighted': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "            'recall_macro': recall_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "            'recall_weighted': recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "            'f1_macro': f1_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "            'f1_weighted': f1_score(y_test, y_pred, average='weighted', zero_division=0),\n",
        "            'matthews_corrcoef': matthews_corrcoef(y_test, y_pred),\n",
        "            'cohen_kappa': cohen_kappa_score(y_test, y_pred),\n",
        "        }\n",
        "\n",
        "        if y_pred_proba is not None and len(np.unique(y_test)) == 2:\n",
        "            metrics['roc_auc'] = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
        "            metrics['log_loss'] = log_loss(y_test, y_pred_proba)\n",
        "        elif y_pred_proba is not None:\n",
        "            metrics['roc_auc_ovr'] = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
        "            metrics['roc_auc_ovo'] = roc_auc_score(y_test, y_pred_proba, multi_class='ovo')\n",
        "            metrics['log_loss'] = log_loss(y_test, y_pred_proba)\n",
        "\n",
        "        # Cross-validation scores\n",
        "        cv_accuracy = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "        cv_f1 = cross_val_score(model, X_train, y_train, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
        "\n",
        "        metrics['cv_accuracy_mean'] = np.mean(cv_accuracy)\n",
        "        metrics['cv_accuracy_std'] = np.std(cv_accuracy)\n",
        "        metrics['cv_f1_mean'] = np.mean(cv_f1)\n",
        "        metrics['cv_f1_std'] = np.std(cv_f1)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def calculate_regression_metrics(self, model, X_train, X_test, y_train, y_test):\n",
        "        \"\"\"Calculate comprehensive regression metrics\"\"\"\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        metrics = {\n",
        "            'mse': mean_squared_error(y_test, y_pred),\n",
        "            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "            'mae': mean_absolute_error(y_test, y_pred),\n",
        "            'mape': mean_absolute_percentage_error(y_test, y_pred),\n",
        "            'r2': r2_score(y_test, y_pred),\n",
        "            'explained_variance': explained_variance_score(y_test, y_pred),\n",
        "            'median_ae': median_absolute_error(y_test, y_pred),\n",
        "            'max_error': max_error(y_test, y_pred),\n",
        "        }\n",
        "\n",
        "        # Additional metrics if suitable\n",
        "        if np.all(y_test > 0) and np.all(y_pred > 0):\n",
        "            try:\n",
        "                metrics['mean_poisson_deviance'] = mean_poisson_deviance(y_test, y_pred)\n",
        "                metrics['mean_gamma_deviance'] = mean_gamma_deviance(y_test, y_pred)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Cross-validation scores\n",
        "        cv_r2 = cross_val_score(model, X_train, y_train, cv=3, scoring='r2', n_jobs=-1)\n",
        "        cv_mae = cross_val_score(model, X_train, y_train, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "\n",
        "        metrics['cv_r2_mean'] = np.mean(cv_r2)\n",
        "        metrics['cv_r2_std'] = np.std(cv_r2)\n",
        "        metrics['cv_mae_mean'] = -np.mean(cv_mae)\n",
        "        metrics['cv_mae_std'] = np.std(cv_mae)\n",
        "\n",
        "        # Spearman correlation\n",
        "        metrics['spearman_corr'] = spearmanr(y_test, y_pred)[0]\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def calculate_clustering_metrics(self, model, X, true_labels=None):\n",
        "        \"\"\"Calculate comprehensive clustering metrics\"\"\"\n",
        "        labels = model.fit_predict(X)\n",
        "\n",
        "        metrics = {\n",
        "            'silhouette_score': silhouette_score(X, labels) if len(np.unique(labels)) > 1 else -1,\n",
        "            'calinski_harabasz_score': calinski_harabasz_score(X, labels) if len(np.unique(labels)) > 1 else -1,\n",
        "            'davies_bouldin_score': davies_bouldin_score(X, labels) if len(np.unique(labels)) > 1 else float('inf'),\n",
        "        }\n",
        "\n",
        "        if true_labels is not None:\n",
        "            metrics.update({\n",
        "                'adjusted_rand_score': adjusted_rand_score(true_labels, labels),\n",
        "                'normalized_mutual_info': normalized_mutual_info_score(true_labels, labels),\n",
        "                'homogeneity_score': homogeneity_score(true_labels, labels),\n",
        "                'completeness_score': completeness_score(true_labels, labels),\n",
        "                'v_measure_score': v_measure_score(true_labels, labels),\n",
        "            })\n",
        "\n",
        "        return metrics, labels\n",
        "\n",
        "    def evaluate_model(self, model, X_train, X_test, y_train, y_test, task_type):\n",
        "        \"\"\"Evaluate a single model with timing\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            if task_type == \"classification\":\n",
        "                metrics = self.calculate_classification_metrics(model, X_train, X_test, y_train, y_test)\n",
        "            elif task_type == \"regression\":\n",
        "                metrics = self.calculate_regression_metrics(model, X_train, X_test, y_train, y_test)\n",
        "            else:  # clustering\n",
        "                metrics, labels = self.calculate_clustering_metrics(model, X_train, y_train)\n",
        "\n",
        "            execution_time = time.time() - start_time\n",
        "            metrics['execution_time'] = execution_time\n",
        "\n",
        "            return metrics, None\n",
        "\n",
        "        except Exception as e:\n",
        "            execution_time = time.time() - start_time\n",
        "            return {'execution_time': execution_time}, str(e)\n",
        "\n",
        "    def evaluate_models_parallel(self, models, X_train, X_test, y_train, y_test, task_type):\n",
        "        \"\"\"Evaluate multiple models in parallel\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=min(4, len(models))) as executor:\n",
        "            # Submit all tasks\n",
        "            future_to_model = {\n",
        "                executor.submit(\n",
        "                    self.evaluate_model,\n",
        "                    model, X_train, X_test, y_train, y_test, task_type\n",
        "                ): name for name, model in models.items()\n",
        "            }\n",
        "\n",
        "            # Collect results as they complete\n",
        "            for future in concurrent.futures.as_completed(future_to_model):\n",
        "                model_name = future_to_model[future]\n",
        "                try:\n",
        "                    metrics, error = future.result()\n",
        "                    results[model_name] = {'metrics': metrics, 'error': error}\n",
        "                except Exception as e:\n",
        "                    results[model_name] = {'metrics': {}, 'error': str(e)}\n",
        "\n",
        "        return results\n",
        "\n",
        "    def compare_algorithms(self, task_type, selected_algorithms, X, y=None):\n",
        "        \"\"\"Compare multiple algorithms efficiently\"\"\"\n",
        "        cache_key = f\"{task_type}_{hash(str(X))}_{hash(str(y)) if y is not None else 0}\"\n",
        "\n",
        "        # Check cache first\n",
        "        if cache_key in self.results_cache:\n",
        "            return self.results_cache[cache_key]\n",
        "\n",
        "        # Preprocess data\n",
        "        if task_type in [\"classification\", \"regression\"]:\n",
        "            X_train, X_test, y_train, y_test = self.preprocess_data(X, y, task_type)\n",
        "        else:\n",
        "            X_train, _, _, _ = self.preprocess_data(X, task_type=task_type)\n",
        "            X_test, y_train, y_test = None, None, None\n",
        "\n",
        "        # Prepare models\n",
        "        models_to_evaluate = {}\n",
        "        for algo in selected_algorithms:\n",
        "            if algo in self.preconfigured_models[task_type]:\n",
        "                models_to_evaluate[algo] = self.preconfigured_models[task_type][algo]\n",
        "\n",
        "        # Evaluate models in parallel\n",
        "        results = self.evaluate_models_parallel(\n",
        "            models_to_evaluate, X_train, X_test, y_train, y_test, task_type\n",
        "        )\n",
        "\n",
        "        # Cache results\n",
        "        self.results_cache[cache_key] = results\n",
        "\n",
        "        return results\n",
        "\n",
        "# Create the comparator instance\n",
        "comparator = ComprehensiveMLComparator()\n",
        "\n",
        "# Create Gradio interface\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=\"Comprehensive ML Algorithm Comparator with XGBoost\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"# ⚡ Comprehensive ML Algorithm Comparator with XGBoost\")\n",
        "        gr.Markdown(\"Compare different ML algorithms with extensive metrics and find the best one for your dataset!\")\n",
        "\n",
        "        # Display XGBoost availability status\n",
        "        if not XGBOOST_AVAILABLE:\n",
        "            gr.Markdown(\"\"\"\n",
        "            <div style='background-color: #fff3cd; border-left: 4px solid #ffc107; padding: 10px; margin: 10px 0;'>\n",
        "            ⚠️ <b>XGBoost not available</b>. Install with: <code>pip install xgboost</code> for enhanced performance.\n",
        "            </div>\n",
        "            \"\"\")\n",
        "        else:\n",
        "            gr.Markdown(\"\"\"\n",
        "            <div style='background-color: #d4edda; border-left: 4px solid #28a745; padding: 10px; margin: 10px 0;'>\n",
        "            ✅ <b>XGBoost available</b> and ready to use for classification and regression tasks.\n",
        "            </div>\n",
        "            \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                task_type = gr.Dropdown(\n",
        "                    choices=[\"classification\", \"regression\", \"clustering\"],\n",
        "                    label=\"Task Type\",\n",
        "                    value=\"classification\"\n",
        "                )\n",
        "\n",
        "                use_sample_data = gr.Checkbox(label=\"Use Sample Data\", value=True)\n",
        "\n",
        "                with gr.Row():\n",
        "                    n_samples = gr.Slider(minimum=100, maximum=5000, value=1000, step=100, label=\"Samples\")\n",
        "                    n_features = gr.Slider(minimum=2, maximum=50, value=10, step=1, label=\"Features\")\n",
        "\n",
        "                file_upload = gr.File(\n",
        "                    label=\"Upload your dataset (CSV)\",\n",
        "                    file_types=[\".csv\"]\n",
        "                )\n",
        "\n",
        "                gr.Markdown(\"### Select Algorithms\")\n",
        "\n",
        "                # Algorithm selection based on task type\n",
        "                algorithm_selection = gr.CheckboxGroup(\n",
        "                    label=\"Algorithms\",\n",
        "                    choices=list(comparator.preconfigured_models['classification'].keys()),\n",
        "                    value=list(comparator.preconfigured_models['classification'].keys())[:4],\n",
        "                    interactive=True\n",
        "                )\n",
        "\n",
        "                # Update algorithm choices when task type changes\n",
        "                def update_algorithm_choices(task_type):\n",
        "                    algorithms = list(comparator.preconfigured_models[task_type].keys())\n",
        "                    # Prioritize XGBoost in default selection if available\n",
        "                    default_algorithms = algorithms[:3]\n",
        "                    if XGBOOST_AVAILABLE and 'xgboost' in algorithms and 'xgboost' not in default_algorithms:\n",
        "                        default_algorithms = ['xgboost'] + default_algorithms[:2]\n",
        "                    return gr.CheckboxGroup(choices=algorithms, value=default_algorithms)\n",
        "\n",
        "                task_type.change(\n",
        "                    update_algorithm_choices,\n",
        "                    inputs=task_type,\n",
        "                    outputs=algorithm_selection\n",
        "                )\n",
        "\n",
        "                # XGBoost parameter tuning (when available and selected)\n",
        "                with gr.Accordion(\"XGBoost Parameters (Advanced)\", open=False) as xgb_params:\n",
        "                    xgb_learning_rate = gr.Slider(0.01, 0.3, value=0.1, step=0.01, label=\"Learning Rate\")\n",
        "                    xgb_max_depth = gr.Slider(1, 10, value=3, step=1, label=\"Max Depth\")\n",
        "                    xgb_subsample = gr.Slider(0.5, 1.0, value=0.8, step=0.1, label=\"Subsample Ratio\")\n",
        "                    xgb_colsample = gr.Slider(0.5, 1.0, value=0.8, step=0.1, label=\"Column Sample Ratio\")\n",
        "\n",
        "                compare_btn = gr.Button(\"Compare Algorithms\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                output_message = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "                with gr.Tab(\"Results Table\"):\n",
        "                    results_df = gr.Dataframe(\n",
        "                        label=\"Comparison Results\",\n",
        "                        headers=[\"Algorithm\", \"Primary Metric\", \"Secondary Metric\", \"Time\", \"Status\"],\n",
        "                        interactive=False,\n",
        "                        wrap=True\n",
        "                    )\n",
        "\n",
        "                with gr.Tab(\"Detailed Metrics\"):\n",
        "                    detailed_metrics = gr.Dataframe(\n",
        "                        label=\"Detailed Metrics\",\n",
        "                        interactive=False,\n",
        "                        wrap=True\n",
        "                    )\n",
        "\n",
        "                with gr.Tab(\"Visualization\"):\n",
        "                    plot_output = gr.Plot(label=\"Performance Comparison\")\n",
        "\n",
        "                with gr.Tab(\"Feature Importance\") as feature_importance_tab:\n",
        "                    feature_importance_plot = gr.Plot(label=\"Feature Importance (Top Algorithms)\")\n",
        "\n",
        "                best_algorithm = gr.Textbox(label=\"Recommended Algorithm\", interactive=False)\n",
        "\n",
        "        # Function to update XGBoost parameters\n",
        "        def update_xgboost_parameters(learning_rate, max_depth, subsample, colsample):\n",
        "            if XGBOOST_AVAILABLE:\n",
        "                # Update classification XGBoost\n",
        "                if 'xgboost' in comparator.preconfigured_models['classification']:\n",
        "                    comparator.preconfigured_models['classification']['xgboost'] = xgb.XGBClassifier(\n",
        "                        n_estimators=50, learning_rate=learning_rate, random_state=42, n_jobs=-1,\n",
        "                        max_depth=max_depth, subsample=subsample, colsample_bytree=colsample\n",
        "                    )\n",
        "\n",
        "                # Update regression XGBoost\n",
        "                if 'xgboost' in comparator.preconfigured_models['regression']:\n",
        "                    comparator.preconfigured_models['regression']['xgboost'] = xgb.XGBRegressor(\n",
        "                        n_estimators=50, learning_rate=learning_rate, random_state=42, n_jobs=-1,\n",
        "                        max_depth=max_depth, subsample=subsample, colsample_bytree=colsample\n",
        "                    )\n",
        "\n",
        "            return \"XGBoost parameters updated successfully!\"\n",
        "\n",
        "        # Connect XGBoost parameter updates\n",
        "        for param in [xgb_learning_rate, xgb_max_depth, xgb_subsample, xgb_colsample]:\n",
        "            param.change(\n",
        "                update_xgboost_parameters,\n",
        "                inputs=[xgb_learning_rate, xgb_max_depth, xgb_subsample, xgb_colsample],\n",
        "                outputs=output_message\n",
        "            )\n",
        "\n",
        "        # Function to process the comparison\n",
        "        def run_comparison(task_type, algorithm_selection, file_upload, use_sample_data, n_samples, n_features):\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Get the data\n",
        "            if use_sample_data:\n",
        "                X, y = comparator.generate_sample_data(task_type, n_samples, n_features)\n",
        "                data_source = f\"Generated {task_type} data with {n_samples} samples and {n_features} features\"\n",
        "            elif file_upload is not None:\n",
        "                try:\n",
        "                    df = pd.read_csv(file_upload.name)\n",
        "                    if task_type == \"clustering\":\n",
        "                        X = df.values\n",
        "                        y = None\n",
        "                    else:\n",
        "                        X = df.iloc[:, :-1].values\n",
        "                        y = df.iloc[:, -1].values\n",
        "                    data_source = f\"Uploaded data with {X.shape[0]} samples and {X.shape[1]} features\"\n",
        "                except Exception as e:\n",
        "                    return f\"Error loading file: {str(e)}\", None, None, None, None, None\n",
        "            else:\n",
        "                return \"Please upload a file or use sample data\", None, None, None, None, None\n",
        "\n",
        "            # Run comparison\n",
        "            results = comparator.compare_algorithms(task_type, algorithm_selection, X, y)\n",
        "\n",
        "            # Process results for summary table\n",
        "            results_list = []\n",
        "            detailed_metrics_list = []\n",
        "            best_score = -float('inf')\n",
        "            best_algo = \"\"\n",
        "\n",
        "            # For feature importance\n",
        "            feature_importance_data = {}\n",
        "\n",
        "            for algo, result in results.items():\n",
        "                if result['error']:\n",
        "                    results_list.append([algo, \"Error\", \"N/A\", \"N/A\", result['error']])\n",
        "                    detailed_metrics_list.append({\"Algorithm\": algo, \"Status\": \"Error\", \"Error\": result['error']})\n",
        "                else:\n",
        "                    metrics = result['metrics']\n",
        "\n",
        "                    # Determine the best metrics based on task type\n",
        "                    if task_type == \"classification\":\n",
        "                        primary_metric = f\"Accuracy: {metrics.get('accuracy', 0):.4f}\"\n",
        "                        secondary_metric = f\"F1: {metrics.get('f1_weighted', 0):.4f}\"\n",
        "                        score = metrics.get('accuracy', 0)\n",
        "                    elif task_type == \"regression\":\n",
        "                        primary_metric = f\"R²: {metrics.get('r2', 0):.4f}\"\n",
        "                        secondary_metric = f\"MAE: {metrics.get('mae', 0):.4f}\"\n",
        "                        score = metrics.get('r2', 0)\n",
        "                    else:  # clustering\n",
        "                        primary_metric = f\"Silhouette: {metrics.get('silhouette_score', 0):.4f}\"\n",
        "                        secondary_metric = f\"CH: {metrics.get('calinski_harabasz_score', 0):.4f}\"\n",
        "                        score = metrics.get('silhouette_score', 0)\n",
        "\n",
        "                    results_list.append([\n",
        "                        algo,\n",
        "                        primary_metric,\n",
        "                        secondary_metric,\n",
        "                        f\"{metrics.get('execution_time', 0):.3f}s\",\n",
        "                        \"Success\"\n",
        "                    ])\n",
        "\n",
        "                    # Add to detailed metrics\n",
        "                    detailed_row = {\"Algorithm\": algo, \"Status\": \"Success\"}\n",
        "                    for k, v in metrics.items():\n",
        "                        if isinstance(v, (int, float)):\n",
        "                            detailed_row[k] = round(v, 6)\n",
        "                        else:\n",
        "                            detailed_row[k] = v\n",
        "                    detailed_metrics_list.append(detailed_row)\n",
        "\n",
        "                    # Track best algorithm\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_algo = algo\n",
        "\n",
        "            # Create performance visualization\n",
        "            fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "            if results_list and any(\"Error\" not in row for row in results_list):\n",
        "                # Extract data for plotting\n",
        "                algorithms = [row[0] for row in results_list if row[4] != \"Error\"]\n",
        "\n",
        "                if task_type == \"classification\":\n",
        "                    scores1 = [float(row[1].split(\": \")[1]) for row in results_list if row[4] != \"Error\"]\n",
        "                    scores2 = [float(row[2].split(\": \")[1]) for row in results_list if row[4] != \"Error\"]\n",
        "                    ylabel1, ylabel2 = \"Accuracy\", \"F1 Score\"\n",
        "                elif task_type == \"regression\":\n",
        "                    scores1 = [float(row[1].split(\": \")[1]) for row in results_list if row[4] != \"Error\"]\n",
        "                    scores2 = [float(row[2].split(\": \")[1]) for row in results_list if row[4] != \"Error\"]\n",
        "                    ylabel1, ylabel2 = \"R² Score\", \"MAE\"\n",
        "                else:  # clustering\n",
        "                    scores1 = [float(row[1].split(\": \")[1]) for row in results_list if row[4] != \"Error\"]\n",
        "                    scores2 = [float(row[2].split(\": \")[1]) for row in results_list if row[4] != \"Error\"]\n",
        "                    ylabel1, ylabel2 = \"Silhouette Score\", \"Calinski-Harabasz Score\"\n",
        "\n",
        "                # Create subplots\n",
        "                x = np.arange(len(algorithms))\n",
        "                width = 0.35\n",
        "\n",
        "                # First metric\n",
        "                bars1 = ax.bar(x - width/2, scores1, width, label=ylabel1, alpha=0.8)\n",
        "                # Second metric (inverted if needed for better visualization)\n",
        "                if task_type == \"regression\":  # For regression, MAE is better when lower\n",
        "                    scores2 = [-s for s in scores2]  # Invert for visualization\n",
        "                    ylabel2 = \"MAE (inverted)\"\n",
        "\n",
        "                bars2 = ax.bar(x + width/2, scores2, width, label=ylabel2, alpha=0.8)\n",
        "\n",
        "                ax.set_ylabel('Scores')\n",
        "                ax.set_title('Algorithm Performance Comparison')\n",
        "                ax.set_xticks(x)\n",
        "                ax.set_xticklabels(algorithms, rotation=45, ha='right')\n",
        "                ax.legend()\n",
        "\n",
        "                # Add value labels on bars\n",
        "                for bars, scores in zip([bars1, bars2], [scores1, scores2]):\n",
        "                    for bar, score in zip(bars, scores):\n",
        "                        height = bar.get_height()\n",
        "                        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                                f'{score:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "                plt.tight_layout()\n",
        "            else:\n",
        "                ax.text(0.5, 0.5, 'No results to display',\n",
        "                        ha='center', va='center', transform=ax.transAxes)\n",
        "                ax.set_title('Algorithm Performance Comparison')\n",
        "\n",
        "            # Create feature importance plot for tree-based models\n",
        "            fig_importance, ax_importance = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "            if task_type in [\"classification\", \"regression\"] and X is not None:\n",
        "                try:\n",
        "                    # Get feature names or create generic ones\n",
        "                    if hasattr(X, 'columns'):\n",
        "                        feature_names = X.columns.tolist()\n",
        "                    else:\n",
        "                        feature_names = [f'Feature_{i}' for i in range(X.shape[1])]\n",
        "\n",
        "                    # Check for tree-based models that have feature importance\n",
        "                    tree_models = []\n",
        "                    for algo in algorithm_selection:\n",
        "                        if algo in results and not results[algo]['error']:\n",
        "                            model = comparator.preconfigured_models[task_type][algo]\n",
        "                            if hasattr(model, 'feature_importances_'):\n",
        "                                # Train the model to get feature importance\n",
        "                                X_train, X_test, y_train, y_test = comparator.preprocess_data(X, y, task_type)\n",
        "                                model.fit(X_train, y_train)\n",
        "                                importance = model.feature_importances_\n",
        "                                tree_models.append((algo, importance))\n",
        "\n",
        "                    if tree_models:\n",
        "                        # Plot feature importance for top models\n",
        "                        n_models = min(3, len(tree_models))\n",
        "                        fig_importance, axes = plt.subplots(n_models, 1, figsize=(12, 4 * n_models))\n",
        "\n",
        "                        if n_models == 1:\n",
        "                            axes = [axes]\n",
        "\n",
        "                        for i, (algo, importance) in enumerate(tree_models[:n_models]):\n",
        "                            # Sort features by importance\n",
        "                            indices = np.argsort(importance)[::-1]\n",
        "                            sorted_importance = importance[indices]\n",
        "                            sorted_features = [feature_names[j] for j in indices]\n",
        "\n",
        "                            # Plot top 10 features\n",
        "                            top_n = min(10, len(sorted_importance))\n",
        "                            axes[i].barh(range(top_n), sorted_importance[:top_n][::-1], align='center')\n",
        "                            axes[i].set_yticks(range(top_n))\n",
        "                            axes[i].set_yticklabels(sorted_features[:top_n][::-1])\n",
        "                            axes[i].set_xlabel('Feature Importance')\n",
        "                            axes[i].set_title(f'Feature Importance - {algo}')\n",
        "\n",
        "                        plt.tight_layout()\n",
        "                    else:\n",
        "                        ax_importance.text(0.5, 0.5, 'No feature importance data available\\n(Tree-based models only)',\n",
        "                                         ha='center', va='center', transform=ax_importance.transAxes)\n",
        "                        ax_importance.set_title('Feature Importance')\n",
        "                except Exception as e:\n",
        "                    ax_importance.text(0.5, 0.5, f'Error generating feature importance: {str(e)}',\n",
        "                                     ha='center', va='center', transform=ax_importance.transAxes)\n",
        "                    ax_importance.set_title('Feature Importance')\n",
        "            else:\n",
        "                ax_importance.text(0.5, 0.5, 'Feature importance not available for clustering',\n",
        "                                 ha='center', va='center', transform=ax_importance.transAxes)\n",
        "                ax_importance.set_title('Feature Importance')\n",
        "\n",
        "            total_time = time.time() - start_time\n",
        "\n",
        "            return (\n",
        "                f\"{data_source}. Comparison completed in {total_time:.2f} seconds.\",\n",
        "                results_list,\n",
        "                pd.DataFrame(detailed_metrics_list),\n",
        "                fig,\n",
        "                fig_importance,\n",
        "                f\"{best_algo} (Score: {best_score:.4f})\"\n",
        "            )\n",
        "\n",
        "        # Set up event handler\n",
        "        compare_btn.click(\n",
        "            fn=run_comparison,\n",
        "            inputs=[task_type, algorithm_selection, file_upload, use_sample_data, n_samples, n_features],\n",
        "            outputs=[output_message, results_df, detailed_metrics, plot_output, feature_importance_plot, best_algorithm]\n",
        "        )\n",
        "\n",
        "        # Examples for quick testing\n",
        "        gr.Markdown(\"### Quick Examples\")\n",
        "        with gr.Row():\n",
        "            gr.Examples(\n",
        "                examples=[\n",
        "                    [\"classification\", [\"xgboost\", \"random_forest\", \"gradient_boosting\"], True, 1000, 10],\n",
        "                    [\"regression\", [\"xgboost\", \"random_forest\", \"gradient_boosting\"], True, 1000, 10],\n",
        "                    [\"clustering\", [\"kmeans\", \"agglomerative\", \"birch\"], True, 1000, 10]\n",
        "                ],\n",
        "                inputs=[task_type, algorithm_selection, use_sample_data, n_samples, n_features],\n",
        "                label=\"Click any example to quickly set up the interface\"\n",
        "            )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Create and launch the interface\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_interface()\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "hsU9hRsCn2p1",
        "outputId": "791fc192-7f52-40f2-812b-8cc22e755c08"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3049386ad897d385d7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3049386ad897d385d7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}